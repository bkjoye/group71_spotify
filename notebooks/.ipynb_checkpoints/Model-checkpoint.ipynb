{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Modelling\n",
    "nav_include: 5\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling\n",
    "\n",
    "The goal is to create a playlist, starting from one song, as this could be easily extended to starting from multiple songs. Dataset is split into three parts:\n",
    "\n",
    "* Training\n",
    "* Validation\n",
    "* Test\n",
    "\n",
    "(please see \"Splitting Data\" page for details)\n",
    "\n",
    "We use the following three components for modelling:\n",
    "\n",
    "* Similar songs lookup (recursive)\n",
    "* Regression model that estimates number of followers (coefficients calculated based on training dataset)\n",
    "* KMeans clustering on playlist engineered features to add songs from playlists which belong to the same centroid as the \"base\" playlist. \"Top\" similar (belonging to the same cluster, ordered by number of followers, descending) playlists are used to supply songs (chosen at random)\n",
    "\n",
    "`Metrics`:\n",
    "* Find which songs generated and original playlists have in common and calculate number of songs which code guessed correctly \n",
    "* Calculate aggregated (engineered) values from songs to generated playlist and come up with (Euclidean) \"distance\" between generated playlist and original playlist\n",
    "* Using regression model that was fit on training data, predict number of followers for generated playlist and calculate how different it is from true num_followers\n",
    "* Sum these metrics together to find the loss but invert number of song matches to make sure smaller metric is better\n",
    "* Alternative summary metric(\"metric2\" in Modelling Results): we found out that resulting summary metric has large variance. Majority of this variance comes from number of followers estimation. Metric2 includes only inverted number of song matches plus distance of generated playlist's engineered features to the original playlist. This metric has much lower variance\n",
    "\n",
    "\n",
    "`Metaparameters`\n",
    "* Number of clusters. We tried 2 / 10 / 50 / 100 cluster splits\n",
    "* Number of playlists (5) to choose songs from the same cluster. Refers to taking songs from only 5 playlists which came from closest cluster (ordered by num_followers, descending)\n",
    "* Number of similar songs (10) to fetch at each step. Similar songs are added recursively (i.e. add 10 songs, go through them to find similar songs for each in order) - until there are enough or none can be taken\n",
    "* Algorithm fills 50% of playlist from similar songs and 50% from clusters. If there were only a few similar songs for the first phase - similar songs are also taken from the list of songs from clusters\n",
    "\n",
    "It would be great to run through several metaparam variations but processing time is too high to try that out. Only variation of number of clusters was done. There was not enough time to find out if splitting similar songs / playlists should be done not 50 / 50 but in different proportion.\n",
    "After training and validation, it looks like best number of clusters are 2 and 10. On average, 10 clusters seems to be best.\n",
    "\n",
    "`Execution`\n",
    "* It turned out that playlist generation is pretty slow - running through 500 playlists takes an hour. Hence, code splits (after shuffling) the input dataset into \"batches\" and runs analysis in parallel\n",
    "* Each run (train / validation / test) saves results into separate compressed csv files - by batch and by cluster size metaparameter\n",
    "\n",
    "`Programmatically`, code is very similar for train / validation and test. Major differences:\n",
    "* Regression model for number of followers is trained based on training dataset and applied for all datasets\n",
    "* Training is limited to 1000 playlists in each of 6 threads calculated in paralllel. This allows validation and training datasets sizes to be roughly equivalent. Below is the script for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-\n",
    "\n",
    "# Training script\n",
    "\n",
    "import sys\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import gzip\n",
    "import csv\n",
    "from multiprocessing import Process\n",
    "from sklearn.utils import shuffle \n",
    "\n",
    "DATA_DIR=\"./data/data\"\n",
    "\n",
    "df = pd.read_csv(DATA_DIR + '/pidpos.csv.gz', compression='gzip').drop(['Unnamed: 0'],axis=1)\n",
    "dfAugSongs = pd.read_csv(DATA_DIR + '/full_aug_songs.csv.gz', compression='gzip')\n",
    "dfPlaylists = pd.read_csv(DATA_DIR + '/playlists.csv.gz', compression='gzip')\n",
    "dfTrain = pd.read_csv(DATA_DIR + '/train_aug_playlists.csv.gz', compression='gzip').drop(['Unnamed: 0'],axis=1)\n",
    "# For validation, added dfValidate = pd.read_csv(DATA_DIR + '/validate_aug_playlists.csv.gz', compression='gzip').drop(['Unnamed: 0'],axis=1)\n",
    "# For test: dfTest = pd.read_csv(DATA_DIR + '/test_aug_playlists.csv.gz', compression='gzip').drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "dfSim = pd.read_csv(DATA_DIR + '/simsong5.csv.gz', compression='gzip').drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "def addSim(dfSim, cur_set, c_id, num) :\n",
    "    dfCandidates = dfSim[(dfSim.songid == c_id) | (dfSim.simsongid == c_id)]\n",
    "    t = dfCandidates.sort_values(by='count', ascending=False).values[0:num, :]\n",
    "    for i in t :\n",
    "        id = i[1] if i[0] == c_id else i[0]\n",
    "        if id in cur_set :\n",
    "            continue\n",
    "        cur_set.append(id)\n",
    "    return cur_set\n",
    "\n",
    "def getPlAgg(candidate_pl) :\n",
    "     return [ candidate_pl.danceability.mean(), candidate_pl.energy.mean(), \n",
    "                     candidate_pl.speechiness.mean(), candidate_pl.acousticness.mean(), \n",
    "                     candidate_pl.instrumentalness.mean(), candidate_pl.liveness.mean(),\n",
    "                     candidate_pl.valence.mean(), candidate_pl.duration.mean(), candidate_pl.key.max(), \n",
    "                     candidate_pl.loudness.max(), \n",
    "                     candidate_pl.tempo.max(), candidate_pl.time_signature.max() ]# .iloc[0] -> first\n",
    "\n",
    "def getNumfXy(dfPlSongsAgg) :\n",
    "    y = dfPlSongsAgg.num_followers\n",
    "    X = dfPlSongsAgg[['mean_danceability','mean_energy','mean_speechiness','mean_acousticness',\n",
    "                              'mean_instrumentalness', 'mean_liveness', 'mean_valence',\n",
    "                 'mean_duration', 'max_key', 'max_loudness', 'max_tempo', 'max_time_signature']].values\n",
    "    return (X, y)\n",
    "\n",
    "def getNumfRegr(dfPlSongsAgg) :\n",
    "    (X, y) = getNumfXy(dfPlSongsAgg)\n",
    "    return LinearRegression().fit(X, y)\n",
    "\n",
    "def scoreNumfRegr(reg, dfValidate) :\n",
    "    (X, y) = getNumfXy(dfValidate)\n",
    "    return reg.score()\n",
    "\n",
    "def getSongsFrom(dfSongMatch, n) :\n",
    "    return dfSongMatch.sample(n, random_state=0)['track_id'].values.tolist()\n",
    "\n",
    "def getSimSongs(dfSim, song_set, start_num, from_i, n) :\n",
    "    size = len(song_set)\n",
    "    while True :\n",
    "        if size > n or from_i > (size - 1):\n",
    "            break\n",
    "        addSim(dfSim, song_set, song_set[from_i], start_num)\n",
    "        from_i +=1\n",
    "    song_set = song_set[0: n]\n",
    "    assert len(set(song_set)) == len(song_set) # no dups expected\n",
    "    return song_set\n",
    "                    \n",
    "cluster_columns = ['mean_danceability','mean_energy','mean_speechiness','mean_acousticness',\n",
    "                          'mean_instrumentalness', 'mean_liveness', 'mean_valence',\n",
    "             'mean_duration', 'max_key', 'max_loudness', 'max_tempo', 'max_time_signature']\n",
    "\n",
    "dfTrain.dropna(inplace=True)\n",
    "reg = getNumfRegr(dfTrain) # training\n",
    "dfTrain=shuffle(dfTrain)\n",
    "# For validation: dfValidation=shuffle(dfTrain)\n",
    "# For test: dfTest=shuffle(dfTrain)\n",
    "\n",
    "# start_num : how many similar songs to fetch on each level\n",
    "def generate_playlists(dfPlSongsAgg, dfPidPosPl, name, reg, try_clusters, try_startNum, song_name) :\n",
    "    num_top_pl = 5 # choose top 5 playlists\n",
    "    train_song_id = 0 # first song is used to continue playlist\n",
    "    toCluster = dfPlSongsAgg[cluster_columns].values\n",
    "    for n_clusters in try_clusters :\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_jobs = -1).fit(toCluster)\n",
    "        print(\"Found\", n_clusters, \"clusters\")\n",
    "        train_pl = dfPlSongsAgg.playlist_id.values\n",
    "        for start_num in try_startNum :\n",
    "            print(\"start_num\", start_num)\n",
    "            with gzip.open(DATA_DIR + \"/result_\" + name + \"_\" + str(n_clusters) + \"_\" + str(start_num) + \".csv.gz\", \n",
    "                           'wt', newline='') as fz:\n",
    "                writer = csv.writer(fz, delimiter=',')\n",
    "                writer.writerow(['playlist_id', 'n_clusters', 'start_num', 'metric', 'match', 'distance', 'numf', \n",
    "                                 'diff'])\n",
    "                for pl_id in train_pl :\n",
    "                    print(\"Running playlist\", pl_id)\n",
    "                    try :\n",
    "                        train_numf = dfPlSongsAgg[dfPlSongsAgg.playlist_id == pl_id].num_followers.values\n",
    "                        target_n = int(dfPlSongsAgg[dfPlSongsAgg.playlist_id == pl_id].sum_num_tracks.values)\n",
    "                        (train_agg_info, _) = getNumfXy(dfPlSongsAgg[dfPlSongsAgg.playlist_id == pl_id])\n",
    "                        train_song_set = df[df.playlist_id == pl_id].track_id.values \n",
    "                        root_name = dfAugSongs[dfAugSongs.id == train_song_set[train_song_id]].name.values[0]\n",
    "                        fromsim_n = int(target_n / 2) # metaparam\n",
    "                        root_id = dfAugSongs[dfAugSongs.name == root_name].id.values[0]\n",
    "                        song_set = [train_song_set[0]]\n",
    "                        addSim(dfSim, song_set, root_id, start_num)\n",
    "                        # choose start_num - loop to fromsim_n\n",
    "                        getSimSongs(dfSim, song_set, start_num, 0, fromsim_n)\n",
    "                        # find song aug data and create playlist\n",
    "                        candidate_pl = dfAugSongs.iloc[song_set, :]\n",
    "                        candidate_pl_agg = getPlAgg(candidate_pl)\n",
    "                        p_label = kmeans.predict([candidate_pl_agg])[0]\n",
    "                        dfPlaylistMatch = dfPlSongsAgg[kmeans.labels_ == p_label]\n",
    "                        dfPlaylistMatchTop = dfPlaylistMatch.sort_values(by='num_followers', ascending=False).head(\n",
    "                            num_top_pl)\n",
    "                        dfSongMatch = pd.merge(dfPidPosPl, dfPlaylistMatchTop, left_on='playlist_id', right_on='playlist_id', \n",
    "                                               how='left').dropna()\n",
    "                        from_cluster = target_n - fromsim_n\n",
    "                        if from_cluster > dfSongMatch.shape[0] : # not enough songs\n",
    "                            song_from_pl = getSongsFrom(dfSongMatch, dfSongMatch.shape[0])\n",
    "                            getSimSongs(dfSim, song_from_pl, start_num, 0, from_cluster)\n",
    "                        else :    \n",
    "                            song_from_pl = getSongsFrom(dfSongMatch, from_cluster)\n",
    "                        song_play = [*song_set, *song_from_pl]\n",
    "                        song_info = dfAugSongs.iloc[song_play, :]\n",
    "                        # metrics\n",
    "                        metric_match = list(set(train_song_set) & set(song_play))\n",
    "                        metric_match_n = len(metric_match)\n",
    "                        song_agg = np.array(getPlAgg(song_info)) # aggregated to playlist\n",
    "                        dist = (song_agg - train_agg_info)**2\n",
    "                        dist = np.sum(dist, axis=1)\n",
    "                        dist = np.sqrt(dist)[0]\n",
    "                        song_numf = round(reg.predict(song_agg.reshape(1, -1))[0])\n",
    "                        numf_diff = int(np.abs(train_numf - song_numf)[0])\n",
    "                        sum_metric = 1.0 / metric_match_n + dist + numf_diff\n",
    "                        writer.writerow([pl_id, n_clusters, start_num, round(sum_metric, 2), metric_match_n, \n",
    "                                         round(dist, 2), song_numf, numf_diff])\n",
    "                    except  Exception as e:\n",
    "                        print(\"Ex playlist\", pl_id, str(e))\n",
    "\n",
    "kT = int(dfTrain.shape[0] / 6) # dfTrain was changed to dfValidation / dfTest for validation / test\n",
    "# For validation, no limit was used as number of records was roughly the same\n",
    "limit = 1000\n",
    "n_cl = [2,10,50,100]\n",
    "def func1() :\n",
    "    print(\"starting 1\")\n",
    "    dfT = dfTrain.iloc[0 : kT, :].sample(limit, random_state=0)\n",
    "    dfP = pd.merge(df, dfT, left_on='playlist_id', right_on='playlist_id', how='left').dropna()\n",
    "    print(\"running 1\")\n",
    "    generate_playlists(dfT, dfP, \"train1\", reg, try_clusters=n_cl, try_startNum=[10])\n",
    "    print(\"done 1\")\n",
    "def func2() :\n",
    "    print(\"starting 2\")\n",
    "    dfT = dfTrain.iloc[kT : 2*kT, :].sample(limit, random_state=0)\n",
    "    dfP = pd.merge(df, dfT, left_on='playlist_id', right_on='playlist_id', how='left').dropna()\n",
    "    print(\"running 2\")\n",
    "    generate_playlists(dfT, dfP, \"train2\", reg, try_clusters=n_cl, try_startNum=[10])\n",
    "    print(\"done 2\")\n",
    "def func3() :\n",
    "    print(\"starting 3\")\n",
    "    dfT = dfTrain.iloc[2*kT : 3*kT, :].sample(limit, random_state=0)\n",
    "    dfP = pd.merge(df, dfT, left_on='playlist_id', right_on='playlist_id', how='left').dropna()\n",
    "    print(\"running 3\")\n",
    "    generate_playlists(dfT, dfP, \"train3\", reg, try_clusters=n_cl, try_startNum=[10])\n",
    "    print(\"done 3\")\n",
    "def func4() :\n",
    "    print(\"starting 4\")\n",
    "    dfT = dfTrain.iloc[3*kT : 4*kT, :].sample(limit, random_state=0)\n",
    "    dfP = pd.merge(df, dfT, left_on='playlist_id', right_on='playlist_id', how='left').dropna()\n",
    "    print(\"running 4\")\n",
    "    generate_playlists(dfT, dfP, \"train4\", reg, try_clusters=n_cl, try_startNum=[10])\n",
    "    print(\"done 4\")\n",
    "def func5() :\n",
    "    print(\"starting 5\")\n",
    "    dfT = dfTrain.iloc[4*kT : 5*kT, :].sample(limit, random_state=0)\n",
    "    dfP = pd.merge(df, dfT, left_on='playlist_id', right_on='playlist_id', how='left').dropna()\n",
    "    print(\"running 5\")\n",
    "    generate_playlists(dfT, dfP, \"train5\", reg, try_clusters=n_cl, try_startNum=[10])\n",
    "    print(\"done 5\")\n",
    "def func6() :\n",
    "    print(\"starting 6\")\n",
    "    dfT = dfTrain.iloc[5*kT :, :].sample(limit, random_state=0)\n",
    "    dfP = pd.merge(df, dfT, left_on='playlist_id', right_on='playlist_id', how='left').dropna()\n",
    "    print(\"running 6\")\n",
    "    generate_playlists(dfT, dfP, \"train6\", reg, try_clusters=n_cl, try_startNum=[10])\n",
    "    print(\"done 6\")\n",
    "\n",
    "p1 = Process(target=func1)\n",
    "p1.start()\n",
    "p2 = Process(target=func2)\n",
    "p2.start()\n",
    "p3 = Process(target=func3)\n",
    "p3.start()\n",
    "p4 = Process(target=func4)\n",
    "p4.start()\n",
    "p5 = Process(target=func5)\n",
    "p5.start()\n",
    "p6 = Process(target=func6)\n",
    "p6.start()\n",
    "p1.join()\n",
    "p2.join()\n",
    "p3.join()\n",
    "p4.join()\n",
    "p5.join()\n",
    "p6.join()\n",
    "\n",
    "print(\"Training done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
